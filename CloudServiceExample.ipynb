{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"XXXXX\"\n",
    "endpoint = \"XXXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing  50  sentences\n",
      "Completed processing  100  sentences\n",
      "Completed processing  150  sentences\n",
      "Completed processing  200  sentences\n",
      "Completed processing  250  sentences\n",
      "Completed processing  300  sentences\n",
      "Completed processing  350  sentences\n",
      "Completed processing  400  sentences\n",
      "Completed processing  450  sentences\n",
      "Completed processing  500  sentences\n",
      "Completed processing  550  sentences\n",
      "Completed processing  600  sentences\n",
      "Completed processing  650  sentences\n",
      "Completed processing  700  sentences\n",
      "Completed processing  750  sentences\n",
      "Completed processing  800  sentences\n",
      "Completed processing  850  sentences\n",
      "Completed processing  900  sentences\n",
      "Completed processing  950  sentences\n",
      "Completed processing  1000  sentences\n"
     ]
    }
   ],
   "source": [
    "testfilepath = \"test_labelled.txt\" #tab seperated file.\n",
    "sentences = []\n",
    "sentiments = [] #0 is negative, 1 is positive\n",
    "preds = [] #0 neg, 1 positive, 2 neutral or mixed\n",
    "preds_dict = {'positive':1, 'negative':0, 'neutral':2, 'mixed':2}\n",
    "count = 0\n",
    "for line in open(testfilepath):\n",
    "    sentence, sentiment = line.strip().split(\"\\t\")\n",
    "    pred = preds_dict[client.analyze_sentiment(documents = [sentence])[0].sentiment]\n",
    "    preds.append(pred)\n",
    "    sentences.append(sentence)\n",
    "    sentiments.append(int(sentiment))\n",
    "    count +=1\n",
    "    if count%50 ==0:\n",
    "        print(\"Completed processing \", count, \" sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(sentiments), len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw=open(\"actual-preds.csv\", \"w\") #writing it out as backup. \n",
    "fw.write(\"actual,predicted\"+\"\\n\")\n",
    "for i in range(0,1000):\n",
    "    fw.write(str(sentiments[i])+\",\"+str(preds[i]))\n",
    "    fw.write(\"\\n\")\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.81      0.88       500\n",
      "           1       0.93      0.88      0.90       500\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.84      1000\n",
      "   macro avg       0.63      0.56      0.59      1000\n",
      "weighted avg       0.95      0.84      0.89      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vajjalas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\vajjalas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\vajjalas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(sentiments, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 473, 0: 420, 2: 107})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print(collections.Counter(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[405  34  61]\n",
      " [ 15 439  46]\n",
      " [  0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(sentiments,preds,labels=[0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
